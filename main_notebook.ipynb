{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7cd11bf",
   "metadata": {},
   "source": [
    "# This ____ does not exist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6386decf",
   "metadata": {},
   "source": [
    "This is a configurable notebook for making a DCGAN on whatever you like!\n",
    "\n",
    "Point to a folder contating raw images of something you want to learn to generate images of, tweak some settings and go!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4edb2504",
   "metadata": {},
   "source": [
    "face_recognition yields poor and slow results so this uses an opencv and mediapipe solution I made. If you plan on using it you will need opencv and mediapipe...\n",
    "\n",
    "Uncomment and run the following code to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06280d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python\n",
    "#!pip install mediapipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9986d3a0",
   "metadata": {},
   "source": [
    "### Edit the variables below then run the cell.\n",
    "Then you can scroll past it and run the training and visualization cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2fa544-bfd1-412c-bf68-93888822afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import glob\n",
    "# Comment this out if not using my face recognition\n",
    "from faceRecognitionModule import FaceDetector\n",
    "# Comment the google colab imports out if not using google colab.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "########################################{ CLEANING AND STORAGE SETTINGS }########################################\n",
    "# Change these how you wish.\n",
    "raw_path = 'drive/MyDrive/raw_imgs'\n",
    "clean_path = 'drive/MyDrive/clean_imgs'\n",
    "npy_path = 'drive/MyDrive/GAN_npy'\n",
    "gen_path = 'drive/MyDrive/generator'\n",
    "\n",
    "# Size of generated and resized images. MUST BE FACTOR OF 8!\n",
    "# Default is 64,64. If you go much larger you will want to change GAN architecture!\n",
    "# Keep 1:1 ratio!\n",
    "img_size = (64,64)\n",
    "img_shape = (img_size[0],img_size[1],3) # leave this alone\n",
    "\n",
    "# Save .npy array to directory?\n",
    "save_npy = True\n",
    "\n",
    "# Use face detection to crop images to face?\n",
    "crop_to_face = True\n",
    "\n",
    "# simple crop and scale to target size (center crop to 1:1 ratio then scales down. Be mindful that important features don't get cut off)\n",
    "simple_crop = False\n",
    "\n",
    "# Simply resize the images (be mindful of original resolution to avoid 'squished images')\n",
    "simple_resize = False\n",
    "### ONLY ONE IS USED, RESPECTIVE PRIORITY!!! if crop_to_face == True: nothing else will execute!\n",
    "\n",
    "# Pad size on face cropping (fraction) EX: 5 = 1/5 of original size is added to cropped image.\n",
    "# (only relevant if crop_to_face == True)\n",
    "pad_size = 5\n",
    "\n",
    "########################################{ GAN TRAINING SETTINGS }########################################\n",
    "\n",
    "# Latent noise dim, default 128\n",
    "latent_dim = 128\n",
    "\n",
    "# Epochs (Tweak this depending on the size of your dataset, very small datasets will require many many epochs and very large will require not nearly as many)\n",
    "epochs = 30\n",
    "\n",
    "# Learning rate, expirement with this if you're getting many artifacts or can want to try get away with fewer epochs\n",
    "learning_rate = 0.0001\n",
    "\n",
    "########################################{ END OF HIGH LEVEL USER CONFIGURATION }#########################\n",
    "\n",
    "\n",
    "\n",
    "# You shouldn't need to change anything else, run this cell and go to the next.\n",
    "\n",
    "\n",
    "################################################################{ CLEANING }################################################################\n",
    "\n",
    "def detect_face(image):\n",
    "    im, bbox = detector.findFaces(np.array(image),draw=False)\n",
    "\n",
    "    if len(bbox) > 0: # if face was detected\n",
    "        # get crop coords\n",
    "        x, y, w, h = bbox[0][0]\n",
    "\n",
    "        # pad them\n",
    "        pad = np.min([image.size[0],image.size[1]]) // pad_size\n",
    "        x -= pad\n",
    "        y -= pad\n",
    "        x1, y1, = x+w, y+h\n",
    "        x1 += pad\n",
    "        y1 += pad\n",
    "            \n",
    "        # do some ugly padding management to avoid overshoot\n",
    "        if x < 0: x = 0\n",
    "        if y < 0: y = 0\n",
    "        if x > image.size[0]: x = image.size[0]\n",
    "        if y > image.size[1]: y = image.size[1]\n",
    "        if x1 > image.size[0]: x1 = image.size[0]\n",
    "        if y1 > image.size[1]: y1 = image.size[1]\n",
    "        if x1 < 0: x1 = 0\n",
    "        if y1 < 0: y1 = 0\n",
    "            \n",
    "        # simple crop\n",
    "        im = im[y:y1,x:x1]\n",
    "\n",
    "        # anti alias scale to desired ratio\n",
    "        im = Image.fromarray(im)\n",
    "        im = im.resize(img_size,Image.ANTIALIAS)\n",
    "            \n",
    "        im.save(f\"{clean_path}/img_{i}.jpg\")\n",
    "            \n",
    "        print(f'processed image {i}', end='\\r')\n",
    "    else: # if face is not detected we skip\n",
    "        print(f\"Rejected image {i}\", end='\\r')\n",
    "\n",
    "def simple_resizer(im):\n",
    "    im = im.resize(img_size,Image.ANTIALIAS)\n",
    "            \n",
    "    im.save(f\"{clean_path}/img_{i}.jpg\")\n",
    "    print(f'processed image {i}', end='\\r')\n",
    "\n",
    "def simple_cropper(img):\n",
    "    width, height = img.size\n",
    "    # 1:1 crop calc\n",
    "    if width > height:\n",
    "        left = (width - height) / 2\n",
    "        right = left + height\n",
    "        top = 0\n",
    "        bottom = height\n",
    "    elif width < height:\n",
    "        top = (height - width) / 2\n",
    "        bottom = top + width\n",
    "        left = 0\n",
    "        right = width\n",
    "\n",
    "    img = img.crop((left, top, right, bottom))\n",
    "    img = img.resize(img_size, Image.ANTIALIAS)\n",
    "    img.save(f\"{clean_path}/img_{i}.jpg\")\n",
    "    print(f'processed image {i}', end='\\r')\n",
    "\n",
    "# Avoid error if we're not using it.\n",
    "if crop_to_face: detector = FaceDetector()\n",
    "\n",
    "# process images\n",
    "for i,img in enumerate(glob.glob(raw_path+'*')):\n",
    "    image = Image.open(img)\n",
    "    image = ImageOps.exif_transpose(image)\n",
    "    if crop_to_face: detect_face(image)\n",
    "    elif simple_resize: simple_resizer(image)\n",
    "    elif simple_crop: simple_cropper(image)\n",
    "\n",
    "\n",
    "# add cleaned images to np.array to feed to our GAN.\n",
    "image_array = np.zeros((len(glob.glob(clean_path+'/*')), img_size[0], img_size[1], 3), dtype=np.float32)\n",
    "for i, image_path in enumerate(glob.glob(clean_path+'/*')):\n",
    "    with Image.open(image_path) as img:\n",
    "        img = np.array(img)\n",
    "        image_array[i] = (img / 127.5) - 1.0\n",
    "        print(f'Loaded image #{i+1}', end='\\r')\n",
    "\n",
    "if save_npy: np.save(npy_path+'/imgs.npy',image_array)\n",
    "\n",
    "################################################################{ CLEANING END }################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################{ GAN BEGIN }###################################################################\n",
    "\n",
    "\n",
    "# modified Keras DCGAN\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense((img_size[0] // 8) * (img_size[0] // 8) * 128),\n",
    "        layers.Reshape(((img_size[0] // 8), (img_size[0] // 8), 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"tanh\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=img_shape),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b77e875f",
   "metadata": {},
   "source": [
    "### Where the magic happens\n",
    "\n",
    "once that finishes running you can run the cell below to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "676ac57e-6449-4c02-aaf2-90602b5edc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.fit(\n",
    "    image_array, epochs=epochs\n",
    ")#                        ^  swap this out with another number if you want to do more or less epochs..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78492c66",
   "metadata": {},
   "source": [
    "And run this cell to see what sort of images it's generating.\n",
    "\n",
    "You'll need to expirement around a bit to see how many epochs to do for the size of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680cb9b-32f3-4b40-9141-f672ca9ff4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_latent_vectors = tf.random.normal(shape=(1, gan.latent_dim))\n",
    "generated_images = gan.generator(random_latent_vectors)\n",
    "import matplotlib.pyplot as plt\n",
    "# ugly one liner that goes from [-1,1] eagle tensor float to [0,255] numpy int\n",
    "plt.imshow(((np.array(generated_images[0])+1)*127.5).astype(np.uint8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36238ff9",
   "metadata": {},
   "source": [
    "Save the generator if you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f317c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.generator.save(gen_path+'/generator.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
